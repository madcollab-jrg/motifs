---
title: "Motif"
author: "Corey Jackson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(readr)
library(stringi)
library(lme4)
library(performance)
library(dplyr)
library(tidyverse)
library(scales)
library(correlation) # Do by level
library(broom)
library(rcompanion)
library(stargazer)
library(sjPlot) #http://www.strengejacke.de/sjPlot/articles/sjtlm.html 
library(sjmisc)
library(sjlabelled)
library(caret)

#Functions
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

stargazer2 <- function(model, odd.ratio = F, ...) {
  if(!("list" %in% class(model))) model <- list(model)
    
  if (odd.ratio) {
    coefOR2 <- lapply(model, function(x) exp(coef(x)))
    seOR2 <- lapply(model, function(x) exp(coef(x)) * summary(x)$coef[, 2])
    p2 <- lapply(model, function(x) summary(x)$coefficients[, 4])
    stargazer(model, coef = coefOR2, se = seOR2, p = p2, ...)
    
  } else {
    stargazer(model, ...)
  }
}


motifs <- data.table::fread("~/Library/CloudStorage/Box-Box/_research/traceGA/motif[collab]/data/motifs-regression.csv")

# transform datasets
motifs$level <- as.numeric(stri_sub(motifs$Promotion_Level,-1,-1))
motifs$level <- motifs$level-1
motifs$prom <- stri_sub(motifs$Promotion_Level,1,1)
motifs$promoted <- ifelse(motifs$prom=="N",0,1) 
motifs$prom <- NULL 
#motifs$Promotion_Level <- NULL 
motifs <- motifs[which(motifs$level>0 & motifs$level < 5 & motifs$level !=0),]

motifs <- motifs %>% 
        dplyr::rename("Explore" = "exploring",
               "Discuss-H" = "viewdiscussions_high",
               "Classify-M" = "classify_mid",
               "Learn" = "learning",
               "Personal" = "personal",
               "ClassifyH" = "classify_high",
               "ClassifyL" = "classify_low",
               "Social" = "socialize",
               "DiscussM" = "viewdiscussions_normal",
               "CommunalM" = "communal_normal",
               "CommunalH" = "communal_high",
               "Discuss-M" = "viewdiscussions_normal",
               "Communal-M" = "communal_normal",
               "Communal-H" = "communal_high"
               )

##### Correlation analysis https://statsandr.com/blog/correlation-coefficient-and-correlation-test-in-r/ 

# Scale values
motifs2 <- motifs %>% dplyr::summarise(across(c(19,3,5:7,9,10,12:16,18), scale))
motifs3 <- motifs[,c(19,3,5:7,9,10,12:16,18)]
motifs3 <- data.frame(motifs3)

corr_table <- correlation(motifs3,
  include_factors = TRUE, method = "auto")

corr_table_level <- motifs3 %>%
  group_by(level) %>%
  group_modify(~ correlation(.x, include_factors = TRUE, method = "auto"))

```

## Correlation
```{r correlations, include=FALSE}
library(reshape2)
library(gtsummary)
 
motifs3[motifs3 == 0] <- NA
motifs3.m <- melt(motifs3, id.vars="level")
motifs3.m <- motifs3.m[which(motifs3.m$value!=0),]

library(dplyr)
activities <- motifs3.m %>%
  filter(variable != "EOS") %>%
  tbl_cross(
    row = variable,
    col = level,
    percent = "cell"
    ) 

```

```{r}
activities
```

## Models 
```{r models, include=FALSE, cache=TRUE, warning=FALSE}
# Run regressions (which motifs predict promotion). Shoudl be mixed effects
# https://stats.oarc.ucla.edu/r/dae/mixed-effects-logistic-regression/#:~:text=Mixed%20effects%20logistic%20regression%20is,both%20fixed%20and%20random%20effects or zero-inflated models (https://drizopoulos.github.io/GLMMadaptive/articles/ZeroInflated_and_TwoPart_Models.html)

#https://fukamilab.github.io/BIO202/04-C-zero-data.html
#https://stats.oarc.ucla.edu/r/dae/zinb/
# https://www.r-bloggers.com/2018/06/bias-adjustment-for-rare-events-logistic-regression-in-r/ (TRY)

motifs$userID <- as.factor(motifs$userID)
motifs$level <- as.factor(motifs$level)

M_all <- glmer(promoted ~ Explore + Learn + Personal + Social + ClassifyL + ClassifyM +ClassifyH + DiscussM + DiscussH + CommunalM + CommunalH + 
    (1 | level) + (1 | userID), 
    data = motifs, family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))

#####################
###### LEVEL 1 ###### 
#####################
motifs_l1 <- motifs[which(motifs$level=="1"),]

trainIndex_l1 <- createDataPartition(motifs_l1$promoted, p = .7,
                                  list = FALSE,
                                  times = 1)
train_l1 <- motifs_l1[ trainIndex_l1,]
valid_l1 <- motifs_l1[-trainIndex_l1,]

M1 <- glm(promoted ~ Explore + Learn + Personal + Social + ClassifyL + ClassifyM +ClassifyH + DiscussM + DiscussH + CommunalM + CommunalH, data = train_l1, family = "binomial")

logit2prob(coef(M1))
nagelkerke(M1) # performance 
summary(M1)

# compute odds ratio
require(MASS)
exp(cbind(coef(M1), confint(M1))) 

predict_l1 <- predict(M1 , newdata = valid_l1, type = 'response')
confusionMatrix(data = factor(as.numeric(predict_l1>0.5)), reference = factor(valid_l1$promoted))



#####################
###### LEVEL 2 ###### 
#####################
motifs_l2 <- motifs[which(motifs$level=="2"),]

trainIndex_l2 <- createDataPartition(motifs_l2$promoted, p = .7,
                                  list = FALSE,
                                  times = 1)
train_l2 <- motifs_l2[ trainIndex_l2,]
valid_l2 <- motifs_l2[-trainIndex_l2,]

M2 <- glm(promoted ~ Explore + Learn + Personal + Social + ClassifyL + ClassifyM +ClassifyH + DiscussM + DiscussH + CommunalM + CommunalH, data = train_l2, family = "binomial")

logit2prob(coef(M2))
nagelkerke(M2) # performance 
summary(M2)

# compute odds ratio
require(MASS)
exp(cbind(coef(M2), confint(M2))) 

predict_l2 <- predict(M2 , newdata = valid_l2, type = 'response')
confusionMatrix(data = factor(as.numeric(predict_l2>0.5)), reference = factor(valid_l2$promoted))


#####################
###### LEVEL 3 ###### 
#####################
motifs_l3 <- motifs[which(motifs$level=="3"),]

trainIndex_l3 <- createDataPartition(motifs_l3$promoted, p = .7,
                                  list = FALSE,
                                  times = 1)
train_l3 <- motifs_l3[ trainIndex_l3,]
valid_l3 <- motifs_l3[-trainIndex_l3,]

M3 <- glm(promoted ~ Explore + Learn + Personal + Social + ClassifyL + ClassifyM +ClassifyH + DiscussM + DiscussH + CommunalM + CommunalH, data = train_l3, family = "binomial")

logit2prob(coef(M3))
nagelkerke(M3) # performance 
summary(M3)

# compute odds ratio
require(MASS)
exp(cbind(coef(M3), confint(M3))) 

predict_l3 <- predict(M3 , newdata = valid_l3, type = 'response')
confusionMatrix(data = factor(as.numeric(predict_l3>0.5)), reference = factor(valid_l3$promoted))
#####################
###### LEVEL 4 ###### 
#####################

motifs_l4 <- motifs[which(motifs$level=="4"),]

trainIndex_l4 <- createDataPartition(motifs_l4$promoted, p = .7,
                                  list = FALSE,
                                  times = 1)
train_l4 <- motifs_l4[ trainIndex_l4,]
valid_l4 <- motifs_l4[-trainIndex_l4,]

M4 <- glm(promoted ~ Explore + Learn + Personal + Social + ClassifyL + ClassifyM +ClassifyH + DiscussM + DiscussH + CommunalM + CommunalH, data = train_l4, family = "binomial")

logit2prob(coef(M4))
nagelkerke(M4) # performance 
summary(M4)

# compute odds ratio
require(MASS)
exp(cbind(coef(M4), confint(M4))) 

predict_l4 <- predict(M4 , newdata = valid_l4, type = 'response')
confusionMatrix(data = factor(as.numeric(predict_l4>0.5)), reference = factor(valid_l4$promoted))



# the coef. is interpreted as the expected change in log odds for a one-unit increase in variable. 
#(exp(coef)-1)*100 to compute % decrease/increase in the odds of outcome, for a one-unit increase in independent variable
# https://bookdown.org/kdonovan125/ibis_data_analysis_r4/introducing-r-and-rstudio.html 
# https://www.jakeruss.com/cheatsheets/stargazer/

```


```{r models-report, results = "asis", echo=FALSE, include=FALSE}
stargazer(M1,M2,M3,M4, type="latex",font.size = "normalsize",
          #add.lines = lapply(1:nrow(mod_stats), function(i) unlist(mod_stats[i, ])),
          covariate.labels = c("Explore","Discuss(H)","Classify(M)", "Learn", "Personal", "Classify(H)","Social","Classify(L)","Discuss(M)","Communal(M)","Communal(H)","Constant"),
          # notes="<small>Data: ESS, Round 9 (United Kingdom)</small>",
          dep.var.labels="Promoted to Level",
           model.names = FALSE,
          omit.stat=c("f", "ser"),
          column.sep.width = "8pt",
          single.row = TRUE,
          column.labels = c("L1 to L2", "L2 to L3", "L3 to L4","L4 to L5")
          )

```

```{r model-report, warning=FALSE}
# https://www.rdocumentation.org/packages/sjPlot/versions/2.8.4/topics/tab_model
tab_model(M1,M2,M3,M4)

```

# Models with oversampling 

ROSE (Random Over-Sampling Examples) aids the task of binary classification in the presence of rare classes. It produces a synthetic, possibly balanced, sample of data simulated according to a smoothed-bootstrap approach.

```{r}
library(ROSE) # for downsampling
train_l1rose <- train_l1 %>%
  dplyr::select(c(promoted,Explore,Learn,Personal,Social,ClassifyL,ClassifyM,ClassifyH,DiscussM,DiscussH,CommunalM,CommunalH))

 data_rosel1 <- ROSE(promoted~. , data=train_l1rose)$data
    # check (im)balance of new data
    table(data_rosel1$promoted)
    
    
# train logistic regression on balanced data
    rose_modell1 <- glm(promoted ~ ., data=data_rosel1, family="binomial")
    # use the trained model to predict test data
    rose_predl1 <- predict(rose_modell1, newdata=valid_l1,
                                type="response")
    
    roc_predl1 <- prediction(predictions = rose_predl1  , labels = valid_l1$promoted)
    roc_perfl1 <- performance(roc_predl1 , "tpr" , "fpr")
        
    plot(roc_perfl1,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7))

    roc.curve(valid_l1$promoted, rose_predl1, add.roc=TRUE, col=3)
    
confusionMatrix(data = factor(as.numeric(rose_predl1>0.45)),
                reference =  factor(valid_l1$promoted))


### 2

train_l2rose <- train_l2 %>%
  dplyr::select(c(promoted,Explore,Learn,Personal,Social,ClassifyL,ClassifyM,ClassifyH,DiscussM,DiscussH,CommunalM,CommunalH))

 data_rosel2 <- ROSE(promoted~. , data=train_l2rose)$data
    # check (im)balance of new data
    table(data_rosel2$promoted)
    
    
# train logistic regression on balanced data
    rose_modell2 <- glm(promoted ~ ., data=data_rosel2, family="binomial")
    # use the trained model to predict test data
    rose_predl2 <- predict(rose_modell2, newdata=valid_l2,
                                type="response")
    
    roc_predl2 <- prediction(predictions = rose_predl2  , labels = valid_l2$promoted)
    roc_perfl2 <- performance(roc_predl2 , "tpr" , "fpr")
        
    plot(roc_perfl2,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7))

    roc.curve(valid_l2$promoted, rose_predl2, add.roc=TRUE, col=3)
    
confusionMatrix(data = factor(as.numeric(rose_predl2>0.5)),
                reference =  factor(valid_l2$promoted))


### 3

train_l3rose <- train_l3 %>%
  dplyr::select(c(promoted,Explore,Learn,Personal,Social,ClassifyL,ClassifyM,ClassifyH,DiscussM,DiscussH,CommunalM,CommunalH))

 data_rosel3 <- ROSE(promoted~. , data=train_l3rose)$data
    # check (im)balance of new data
    table(data_rosel3$promoted)
    
    
# train logistic regression on balanced data
    rose_modell3 <- glm(promoted ~ ., data=data_rosel3, family="binomial")
    # use the trained model to predict test data
    rose_predl3 <- predict(rose_modell3, newdata=valid_l3,
                                type="response")
    
    roc_predl3 <- prediction(predictions = rose_predl3  , labels = valid_l3$promoted)
    roc_perfl3 <- performance(roc_predl3 , "tpr" , "fpr")
        
    plot(roc_perfl3,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7))

    roc.curve(valid_l3$promoted, rose_predl3, add.roc=TRUE, col=3)
    
confusionMatrix(data = factor(as.numeric(rose_predl3>0.5)),
                reference =  factor(valid_l3$promoted))

## 4

train_l4rose <- train_l4 %>%
  dplyr::select(c(promoted,Explore,Learn,Personal,Social,ClassifyL,ClassifyM,ClassifyH,DiscussM,DiscussH,CommunalM,CommunalH))

 data_rosel4 <- ROSE(promoted~. , data=train_l4rose)$data
    # check (im)balance of new data
    table(data_rosel4$promoted)
    
    
# train logistic regression on balanced data
    rose_modell4 <- glm(promoted ~ ., data=data_rosel4, family="binomial")
    # use the trained model to predict test data
    rose_predl4 <- predict(rose_modell4, newdata=valid_l4,
                                type="response")
    
    roc_predl4 <- prediction(predictions = rose_predl4  , labels = valid_l4$promoted)
    roc_perfl4 <- performance(roc_predl4 , "tpr" , "fpr")
        
    plot(roc_perfl4,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7))

    roc.curve(valid_l4$promoted, rose_predl4, add.roc=TRUE, col=3)
    
confusionMatrix(data = factor(as.numeric(rose_predl4>0.5)),
                reference =  factor(valid_l4$promoted))
```





